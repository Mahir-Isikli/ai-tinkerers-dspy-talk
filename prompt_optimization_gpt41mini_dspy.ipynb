{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "712aea9f",
   "metadata": {},
   "source": [
    "# Automatic System Prompt Optimization (DSPy) â€” with **gpt-4.1-mini**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df7f901",
   "metadata": {},
   "source": [
    "This notebook uses DSPy to optimize a *system prompt* for a task, targeting OpenAI's **gpt-4.1-mini**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0214b0e6",
   "metadata": {},
   "source": [
    "## 1) Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9eadc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%pip install -U dspy openai tiktoken\n",
    "\n",
    "import os, re\n",
    "import dspy\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "BASE_MODEL = \"openai/gpt-4.1-mini\"\n",
    "JUDGE_MODEL = \"openai/gpt-4.1\"\n",
    "\n",
    "dspy.configure(lm=dspy.LM(BASE_MODEL))\n",
    "print(\"DSPy:\", dspy.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0962df86",
   "metadata": {},
   "source": [
    "## 2) Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a909ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "import json\n",
    "\n",
    "# Load the dataset from JSON file\n",
    "with open(\"qna_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "# Convert JSON data to dspy.Example objects\n",
    "train_examples = [\n",
    "    dspy.Example(prompt=ex[\"prompt\"], generation=ex[\"generation\"]) \n",
    "    for ex in dataset[\"train\"]\n",
    "]\n",
    "\n",
    "dev_examples = [\n",
    "    dspy.Example(prompt=ex[\"prompt\"], generation=ex[\"generation\"]) \n",
    "    for ex in dataset[\"dev\"]\n",
    "]\n",
    "\n",
    "# Create train and dev sets\n",
    "trainset = [e.with_inputs(\"prompt\") for e in train_examples]\n",
    "devset = [e.with_inputs(\"prompt\") for e in dev_examples]\n",
    "\n",
    "print(f\"Loaded {len(trainset)} training examples and {len(devset)} dev examples\")\n",
    "(len(trainset), len(devset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dc4b9b",
   "metadata": {},
   "source": [
    "## 3) Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c508c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def token_f1(pred, ref):\n",
    "    p = pred.lower().split(); r = ref.lower().split()\n",
    "    if not p or not r: return 0.0\n",
    "    from collections import Counter\n",
    "    cp, cr = Counter(p), Counter(r)\n",
    "    overlap = sum((cp & cr).values())\n",
    "    prec = overlap/len(p); rec = overlap/len(r)\n",
    "    return 0.0 if (prec+rec)==0 else 2*prec*rec/(prec+rec)\n",
    "\n",
    "def concise_qna_metric(example, prediction, trace=None):\n",
    "    out = (prediction.get(\"generation\") or \"\").strip()\n",
    "    ref = (example.get(\"generation\") or \"\").strip()\n",
    "    if not out: return 0.0\n",
    "    # Encourage <= 2 sentences\n",
    "    import re as _re\n",
    "    sentences = [s for s in _re.split(r\"[.!?]+\", out) if s.strip()]\n",
    "    length_pen = 0.0 if len(sentences)<=2 else min(1.0, 0.2*(len(sentences)-2))\n",
    "    return max(0.0, min(1.0, token_f1(out, ref)-length_pen))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41616441",
   "metadata": {},
   "source": [
    "## 4) Minimal program with custom adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ca493f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  # Need to import sys for the adapter\n",
    "\n",
    "class signature(dspy.Signature):\n",
    "    prompt = dspy.InputField()\n",
    "    generation = dspy.OutputField()\n",
    "\n",
    "def format_demos(demos):\n",
    "    s = []\n",
    "    for d in (demos or []):\n",
    "        s.append(f\"\\n# Example\\nUser: {d.inputs.get('prompt','')}\\nAssistant: {d.outputs.get('generation','')}\")\n",
    "    return \"\\n\".join(s)\n",
    "\n",
    "class SimplestAdapter(dspy.Adapter):\n",
    "    def __call__(self, lm, lm_kwargs, signature, demos, inputs):\n",
    "        sys_msg = signature.instructions or \"\"\n",
    "        if demos: sys_msg += \"\\n\" + format_demos(demos)\n",
    "        messages = [\n",
    "            {\"role\":\"system\",\"content\": sys_msg},\n",
    "            {\"role\":\"user\",\"content\": inputs[\"prompt\"]},\n",
    "        ]\n",
    "        outputs = lm(messages=messages, **lm_kwargs)\n",
    "        return [{\"generation\": outputs[0]}]\n",
    "\n",
    "class MyPredict(dspy.Predict):\n",
    "    def __init__(self, signature, **kw):\n",
    "        super().__init__(signature, **kw)\n",
    "        self.adapter = SimplestAdapter()\n",
    "\n",
    "INITIAL_SYSTEM_PROMPT = \"You are concise. Answer correctly in <= 2 sentences.\"\n",
    "my_program = MyPredict(signature)\n",
    "my_program.signature.instructions = INITIAL_SYSTEM_PROMPT\n",
    "print(my_program(prompt=\"Who painted the Mona Lisa?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b84a72c",
   "metadata": {},
   "source": [
    "## 5) Optimize (MIPROv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c319826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the optimizer\n",
    "teleprompter = dspy.MIPROv2(\n",
    "    metric=concise_qna_metric,\n",
    "    auto=\"medium\",  # Can choose between light, medium, and heavy optimization runs\n",
    "    max_bootstrapped_demos=0,  # No few-shot examples (focusing on instruction optimization)\n",
    "    max_labeled_demos=0,\n",
    ")\n",
    "\n",
    "# Optimize program\n",
    "print(\"Optimizing program with MIPROv2...\")\n",
    "my_program_optimized = teleprompter.compile(\n",
    "    my_program,\n",
    "    trainset=trainset,\n",
    "    requires_permission_to_run=False\n",
    ")\n",
    "\n",
    "# Test the optimized program\n",
    "print(\"\\nOptimized result:\")\n",
    "print(my_program_optimized(prompt=\"What is the capital of Germany?\"))\n",
    "\n",
    "# Inspect the optimization history\n",
    "my_program_optimized.inspect_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35faed0",
   "metadata": {},
   "source": [
    "## 6) Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bea211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(program, dataset, metric):\n",
    "    scores = []\n",
    "    for ex in dataset:\n",
    "        # pull input & reference safely from dspy.Example\n",
    "        user_prompt = getattr(ex, \"prompt\", None) or getattr(ex, \"inputs\", {}).get(\"prompt\", \"\")\n",
    "        ref_answer  = getattr(ex, \"generation\", None) or getattr(ex, \"outputs\", {}).get(\"generation\", \"\")\n",
    "\n",
    "        # run program\n",
    "        pred = program(prompt=user_prompt)\n",
    "\n",
    "        # normalize prediction to a dict with \"generation\"\n",
    "        gen = getattr(pred, \"generation\", None)\n",
    "        if gen is None and hasattr(pred, \"as_dict\"):\n",
    "            gen = pred.as_dict().get(\"generation\", \"\")\n",
    "        if gen is None and hasattr(pred, \"toDict\"):\n",
    "            gen = pred.toDict().get(\"generation\", \"\")\n",
    "        if gen is None and hasattr(pred, \"outputs\") and isinstance(pred.outputs, dict):\n",
    "            gen = pred.outputs.get(\"generation\", \"\")\n",
    "        if gen is None:\n",
    "            try:\n",
    "                gen = pred[\"generation\"]  # last resort if subscriptable\n",
    "            except Exception:\n",
    "                gen = str(pred)\n",
    "\n",
    "        ex_dict   = {\"prompt\": user_prompt, \"generation\": ref_answer}\n",
    "        pred_dict = {\"generation\": gen}\n",
    "        scores.append(metric(ex_dict, pred_dict))\n",
    "\n",
    "    return sum(scores) / len(scores) if scores else 0.0\n",
    "\n",
    "base = evaluate(my_program, devset, concise_qna_metric)\n",
    "opt  = evaluate(my_program_optimized, devset, concise_qna_metric)\n",
    "print(\"Base:\", base, \"Optimized:\", opt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6f5f4a",
   "metadata": {},
   "source": [
    "## 7) Export learned system prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zftbk7b88t",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the optimized program for future use\n",
    "my_program_optimized.save(\"optimized_qna_program.json\")\n",
    "print(\"Optimized program saved to optimized_qna_program.json\")\n",
    "\n",
    "# Also save both prompts in a comparison JSON\n",
    "import json\n",
    "\n",
    "prompts_comparison = {\n",
    "    \"original_prompt\": INITIAL_SYSTEM_PROMPT,\n",
    "    \"optimized_prompt\": my_program_optimized.signature.instructions,\n",
    "    \"base_score\": base,\n",
    "    \"optimized_score\": opt,\n",
    "    \"improvement\": opt - base\n",
    "}\n",
    "\n",
    "with open(\"prompt_comparison.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(prompts_comparison, f, indent=2)\n",
    "\n",
    "print(\"\\nPrompt comparison saved to prompt_comparison.json:\")\n",
    "print(json.dumps(prompts_comparison, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
